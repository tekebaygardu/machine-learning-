{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":536866,"status":"error","timestamp":1731088752342,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"},"user_tz":-180},"id":"1IyRAzsy1L5F","outputId":"a2c7f072-38ca-486b-d09b-746411f5e139"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95       771\n","           1       0.75      0.74      0.75       771\n","           2       0.78      0.74      0.76       771\n","           3       0.97      0.99      0.98       771\n","\n","    accuracy                           0.86      3084\n","   macro avg       0.86      0.86      0.86      3084\n","weighted avg       0.86      0.86      0.86      3084\n","\n","[[751  10  10   0]\n"," [ 37 573 151  10]\n"," [ 11 175 572  13]\n"," [  5   1   3 762]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.96      0.94       771\n","           1       0.76      0.74      0.75       771\n","           2       0.78      0.73      0.75       771\n","           3       0.96      0.99      0.97       771\n","\n","    accuracy                           0.86      3084\n","   macro avg       0.85      0.86      0.86      3084\n","weighted avg       0.85      0.86      0.86      3084\n","\n","[[744  16  11   0]\n"," [ 46 573 147   5]\n"," [ 17 161 566  27]\n"," [  4   0   5 762]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.98      0.94       771\n","           1       0.74      0.71      0.73       771\n","           2       0.78      0.73      0.75       771\n","           3       0.95      0.98      0.97       771\n","\n","    accuracy                           0.85      3084\n","   macro avg       0.85      0.85      0.85      3084\n","weighted avg       0.85      0.85      0.85      3084\n","\n","[[758   8   5   0]\n"," [ 56 550 151  14]\n"," [ 10 180 559  22]\n"," [ 10   1   3 757]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","           0       0.24      0.46      0.32        28\n","           1       0.65      0.66      0.65       407\n","           2       0.78      0.73      0.75       579\n","           3       0.13      0.17      0.15        23\n","\n","    accuracy                           0.68      1037\n","   macro avg       0.45      0.51      0.47      1037\n","weighted avg       0.70      0.68      0.69      1037\n","\n","[[ 13  10   5   0]\n"," [ 34 269  97   7]\n"," [  6 134 420  19]\n"," [  1   3  15   4]]\n"]},{"ename":"NameError","evalue":"name 'joblib' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-969ca40e2111>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Optionally: Save the model for later use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'svm_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved to svm_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"]}],"source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import svm\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the SVM model\n","def create_svm_model():\n","    model = svm.SVC(\n","        C=1.0,\n","        kernel='linear',  # You can experiment with 'rbf', 'poly', etc.\n","        decision_function_shape='ovo',  # One-vs-One for multi-class classification\n","        probability=True  # Needed for predict_proba method\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_svm_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the validation set\n","    y_pred_fold = model.predict(X_val_fold)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_pred_fold))\n","\n","    fold += 1\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","\n","# Train final model\n","model_final = create_svm_model()\n","model_final.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = model_final.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Optionally: Save the model for later use\n","joblib.dump(model_final, 'svm_model.pkl')\n","print(\"Model saved to svm_model.pkl\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2-pmUfAV-sw9","outputId":"84b828e9-8533-460a-c735-30b1359f76f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.83      0.86       771\n","           1       0.78      0.48      0.60       771\n","           2       0.56      0.77      0.65       771\n","           3       0.80      0.87      0.83       771\n","\n","    accuracy                           0.74      3084\n","   macro avg       0.76      0.74      0.73      3084\n","weighted avg       0.76      0.74      0.73      3084\n","\n","[[639  13  78  41]\n"," [ 52 371 286  62]\n"," [ 25  90 591  65]\n"," [  6   1  96 668]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.82      0.80       771\n","           1       0.76      0.48      0.59       771\n","           2       0.55      0.73      0.63       771\n","           3       0.77      0.77      0.77       771\n","\n","    accuracy                           0.70      3084\n","   macro avg       0.72      0.70      0.70      3084\n","weighted avg       0.72      0.70      0.70      3084\n","\n","[[634  32  55  50]\n"," [ 77 368 272  54]\n"," [ 61  80 559  71]\n"," [ 46   5 123 597]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.91      0.84       771\n","           1       0.81      0.46      0.59       771\n","           2       0.59      0.64      0.61       771\n","           3       0.71      0.85      0.77       771\n","\n","    accuracy                           0.71      3084\n","   macro avg       0.72      0.71      0.70      3084\n","weighted avg       0.72      0.71      0.70      3084\n","\n","[[698   9  54  10]\n"," [ 84 354 230 103]\n"," [ 57  70 492 152]\n"," [ 61   4  54 652]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","           0       0.22      0.46      0.30        28\n","           1       0.65      0.51      0.57       407\n","           2       0.72      0.69      0.70       579\n","           3       0.10      0.43      0.16        23\n","\n","    accuracy                           0.61      1037\n","   macro avg       0.42      0.53      0.43      1037\n","weighted avg       0.66      0.61      0.63      1037\n","\n","[[ 13   5   8   2]\n"," [ 30 208 143  26]\n"," [ 15 102 400  62]\n"," [  2   4   7  10]]\n","Model saved to decision_tree_model.pkl\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.tree import DecisionTreeClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Decision Tree model\n","def create_decision_tree_model():\n","    model = DecisionTreeClassifier(\n","        criterion='gini',\n","        max_depth=10,  # You can experiment with different depths\n","        random_state=42\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_decision_tree_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the validation set\n","    y_pred_fold = model.predict(X_val_fold)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_pred_fold))\n","\n","    fold += 1\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","\n","# Train final model\n","model_final = create_decision_tree_model()\n","model_final.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = model_final.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(model_final, 'decision_tree_model.pkl')\n","print(\"Model saved to decision_tree_model.pkl\")\n"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.neighbors import KNeighborsClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the KNN model\n","def create_knn_model():\n","    model = KNeighborsClassifier(\n","        n_neighbors=5,  # You can experiment with different values of k\n","        algorithm='auto',  # You can experiment with 'ball_tree', 'kd_tree', etc.\n","        metric='minkowski'  # You can experiment with different distance metrics\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_knn_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the validation set\n","    y_pred_fold = model.predict(X_val_fold)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_pred_fold))\n","\n","    fold += 1\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","\n","# Train final model\n","model_final = create_knn_model()\n","model_final.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = model_final.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(model_final, 'knn_model.pkl')\n","print(\"Model saved to knn_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XFSrLSnAlBn","executionInfo":{"status":"ok","timestamp":1731090207350,"user_tz":-180,"elapsed":127502,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"9866b390-b32c-454b-964e-1f260b8eb0dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.99      0.94       771\n","           1       0.68      0.71      0.69       771\n","           2       0.76      0.62      0.68       771\n","           3       0.96      1.00      0.98       771\n","\n","    accuracy                           0.83      3084\n","   macro avg       0.82      0.83      0.82      3084\n","weighted avg       0.82      0.83      0.82      3084\n","\n","[[764   3   4   0]\n"," [ 66 547 146  12]\n"," [ 16 258 478  19]\n"," [  0   2   1 768]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      1.00      0.94       771\n","           1       0.69      0.70      0.70       771\n","           2       0.76      0.61      0.68       771\n","           3       0.95      1.00      0.98       771\n","\n","    accuracy                           0.83      3084\n","   macro avg       0.82      0.83      0.82      3084\n","weighted avg       0.82      0.83      0.82      3084\n","\n","[[771   0   0   0]\n"," [ 65 543 148  15]\n"," [ 31 246 471  23]\n"," [  0   1   0 770]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      1.00      0.94       771\n","           1       0.73      0.70      0.71       771\n","           2       0.79      0.67      0.73       771\n","           3       0.94      1.00      0.97       771\n","\n","    accuracy                           0.84      3084\n","   macro avg       0.83      0.84      0.84      3084\n","weighted avg       0.83      0.84      0.84      3084\n","\n","[[770   1   0   0]\n"," [ 79 536 139  17]\n"," [ 22 198 519  32]\n"," [  0   1   1 769]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","           0       0.20      0.57      0.30        28\n","           1       0.57      0.64      0.60       407\n","           2       0.74      0.61      0.67       579\n","           3       0.07      0.09      0.08        23\n","\n","    accuracy                           0.61      1037\n","   macro avg       0.40      0.48      0.41      1037\n","weighted avg       0.64      0.61      0.62      1037\n","\n","[[ 16   8   4   0]\n"," [ 38 259 106   4]\n"," [ 25 180 352  22]\n"," [  0   7  14   2]]\n","Model saved to knn_model.pkl\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,  # Number of trees in the forest\n","        criterion='gini',  # Function to measure the quality of a split\n","        max_depth=None,  # Maximum depth of the tree\n","        random_state=42,\n","        n_jobs=-1  # Use all available cores\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_random_forest_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the validation set\n","    y_pred_fold = model.predict(X_val_fold)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_pred_fold))\n","\n","    fold += 1\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","\n","# Train final model\n","model_final = create_random_forest_model()\n","model_final.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = model_final.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(model_final, 'random_forest_model.pkl')\n","print(\"Model saved to random_forest_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TT9BVnZ2Bg31","executionInfo":{"status":"ok","timestamp":1731095644558,"user_tz":-180,"elapsed":144937,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"f62b12ca-fffe-4b58-9c79-0fa8f25bc7c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.98      0.96       771\n","           1       0.79      0.75      0.77       771\n","           2       0.80      0.80      0.80       771\n","           3       0.99      1.00      0.99       771\n","\n","    accuracy                           0.88      3084\n","   macro avg       0.88      0.88      0.88      3084\n","weighted avg       0.88      0.88      0.88      3084\n","\n","[[755  12   4   0]\n"," [ 39 581 147   4]\n"," [  5 142 620   4]\n"," [  0   0   3 768]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.99      0.97       771\n","           1       0.79      0.77      0.78       771\n","           2       0.81      0.80      0.80       771\n","           3       0.99      1.00      1.00       771\n","\n","    accuracy                           0.89      3084\n","   macro avg       0.89      0.89      0.89      3084\n","weighted avg       0.89      0.89      0.89      3084\n","\n","[[761   9   0   1]\n"," [ 35 591 145   0]\n"," [  4 148 615   4]\n"," [  2   0   0 769]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.99      0.96       771\n","           1       0.80      0.75      0.77       771\n","           2       0.81      0.80      0.80       771\n","           3       0.99      0.99      0.99       771\n","\n","    accuracy                           0.88      3084\n","   macro avg       0.88      0.88      0.88      3084\n","weighted avg       0.88      0.88      0.88      3084\n","\n","[[764   5   2   0]\n"," [ 46 578 145   2]\n"," [  6 142 616   7]\n"," [  4   0   2 765]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.39      0.56        23\n","           1       0.41      0.61      0.49        23\n","           2       0.41      0.83      0.55        23\n","           3       0.67      0.09      0.15        23\n","\n","    accuracy                           0.48        92\n","   macro avg       0.62      0.48      0.44        92\n","weighted avg       0.62      0.48      0.44        92\n","\n","[[ 9 13  1  0]\n"," [ 0 14  9  0]\n"," [ 0  3 19  1]\n"," [ 0  4 17  2]]\n","Model saved to random_forest_model.pkl\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,  # Number of trees in the forest\n","        criterion='gini',  # Function to measure the quality of a split\n","        max_depth=None,  # Maximum depth of the tree\n","        random_state=42,\n","        n_jobs=-1  # Use all available cores\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","# Define parameter grid for hyperparameter tuning\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [10, 20, 30, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","model = create_random_forest_model()\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kf, scoring='accuracy', verbose=1, n_jobs=-1)\n","grid_search.fit(X_train_tfidf, y_train)\n","\n","# Print the best parameters and results\n","print(f\"Best parameters found: {grid_search.best_params_}\")\n","print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n","\n","# Train the final model with the best parameters\n","best_model = grid_search.best_estimator_\n","best_model.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = best_model.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(best_model, 'random_forest_model_with_tuning.pkl')\n","print(\"Model saved to random_forest_model_with_tuning.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6ZOwy35EWiM","executionInfo":{"status":"ok","timestamp":1731092831593,"user_tz":-180,"elapsed":420300,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"f91ec630-4882-4476-9483-6a601e90c00b"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best parameters found: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n","Best cross-validation accuracy: 0.8804582792909641\n","              precision    recall  f1-score   support\n","\n","           0       0.23      0.36      0.28        28\n","           1       0.69      0.65      0.67       407\n","           2       0.78      0.80      0.79       579\n","           3       0.12      0.09      0.10        23\n","\n","    accuracy                           0.71      1037\n","   macro avg       0.46      0.47      0.46      1037\n","weighted avg       0.71      0.71      0.71      1037\n","\n","[[ 10  15   3   0]\n"," [ 27 266 111   3]\n"," [  6 101 461  11]\n"," [  1   3  17   2]]\n","Model saved to random_forest_model_with_tuning.pkl\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0e0ZqapDUJY7","executionInfo":{"status":"ok","timestamp":1731138657630,"user_tz":-180,"elapsed":422,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Load the dataset\n","data = pd.read_csv('/content/no tok +pre.csv')\n","\n","# Drop rows where 'description' or 'severity' is NaN\n","data = data.dropna(subset=['description', 'severity'])\n","\n","# Define the input and target attributes\n","X = data['description']\n","y = data['severity']\n","\n","# Perform stratified sampling\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n","for train_index, test_index in split.split(X, y):\n","    strat_train_set = data.iloc[train_index]\n","    strat_test_set = data.iloc[test_index]\n","\n","# Save the splits to separate CSV files\n","strat_train_set.to_csv('/content/stratified_train_data.csv', index=False)\n","strat_test_set.to_csv('/content/stratified_test_data.csv', index=False)\n","\n","print(\"Stratified sampling complete. Training data saved to /content/stratified_train_data.csv\")\n","print(\"Test data saved to /content/stratified_test_data.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTsMBnJiyH8c","executionInfo":{"status":"ok","timestamp":1731138119241,"user_tz":-180,"elapsed":635,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"ca9def7c-6972-40b5-d727-f7c83a606049"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Stratified sampling complete. Training data saved to /content/stratified_train_data.csv\n","Test data saved to /content/stratified_test_data.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","from nltk.corpus import wordnet\n","import nltk\n","\n","nltk.download('wordnet')\n","\n","# Load the training data\n","df = pd.read_csv('/content/stratified_train_data.csv')\n","\n","def get_synonyms(word):\n","    synonyms = []\n","    for syn in wordnet.synsets(word):\n","        for lemma in syn.lemmas():\n","            synonyms.append(lemma.name())\n","    return list(set(synonyms))\n","\n","def insertion(text, n):\n","    words = text.split()\n","    for _ in range(n):\n","        add_word(words)\n","    return ' '.join(words)\n","\n","def add_word(words):\n","    synonyms = []\n","    counter = 0\n","    while len(synonyms) < 1:\n","        random_word = words[random.randint(0, len(words) - 1)]\n","        synonyms = get_synonyms(random_word)\n","        counter += 1\n","        if counter >= 10:\n","            return\n","    random_synonym = synonyms[0]\n","    random_idx = random.randint(0, len(words) - 1)\n","    words.insert(random_idx, random_synonym)\n","\n","def deletion(text, p):\n","    words = text.split()\n","    if len(words) == 1:\n","        return text\n","    new_words = [word for word in words if random.uniform(0, 1) > p]\n","    if len(new_words) == 0:\n","        return random.choice(words)\n","    return ' '.join(new_words)\n","\n","def substitution(text, n):\n","    words = text.split()\n","    new_words = words.copy()\n","    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n","    random.shuffle(random_word_list)\n","    num_replaced = 0\n","    for random_word in random_word_list:\n","        synonyms = get_synonyms(random_word)\n","        if len(synonyms) >= 1:\n","            synonym = random.choice(synonyms)\n","            new_words = [synonym if word == random_word else word for word in new_words]\n","            num_replaced += 1\n","        if num_replaced >= n:  # Only replace up to n words\n","            break\n","    return ' '.join(new_words)\n","\n","def augment_text(text):\n","    augmented_texts = []\n","    augmented_texts.append(insertion(text, n=1))\n","    augmented_texts.append(deletion(text, p=0.1))\n","    augmented_texts.append(substitution(text, n=1))\n","    return augmented_texts\n","\n","# Balance training data\n","max_samples = df['severity'].value_counts().max()\n","balanced_data = []\n","\n","for severity, group in df.groupby('severity'):\n","    while len(group) < max_samples:\n","        augmented_texts = []\n","        for index, row in group.iterrows():\n","            augmented_texts.extend(augment_text(row['description']))\n","            if len(augmented_texts) >= max_samples - len(group):\n","                break\n","        augmented_df = pd.DataFrame({'description': augmented_texts[:max_samples - len(group)], 'severity': severity})\n","        group = pd.concat([group, augmented_df], ignore_index=True)\n","    balanced_data.append(group)\n","\n","balanced_df = pd.concat(balanced_data, ignore_index=True)\n","\n","# Print class distribution to make sure it is balanced\n","print(\"Class distribution in the balanced training data:\")\n","print(balanced_df['severity'].value_counts())\n","\n","# Save the balanced and augmented training data to a new CSV file\n","balanced_df.to_csv('/content/augmented_balanced_train_data.csv', index=False)\n","\n","print(\"Data augmentation and balancing complete. Saved to /content/augmented_balanced_train_data.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U22K-bHK5FOE","executionInfo":{"status":"ok","timestamp":1731141743290,"user_tz":-180,"elapsed":2044,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"ad5e770a-01fb-4f17-8625-f7fda7453975"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Class distribution in the balanced training data:\n","severity\n","0.0    1850\n","1.0    1850\n","2.0    1850\n","3.0    1850\n","Name: count, dtype: int64\n","Data augmentation and balancing complete. Saved to /content/augmented_balanced_train_data.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/stratified_test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,  # Number of trees in the forest\n","        criterion='gini',  # Function to measure the quality of a split\n","        max_depth=None,  # Maximum depth of the tree\n","        random_state=42,\n","        n_jobs=-1  # Use all available cores\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_random_forest_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the validation set\n","    y_pred_fold = model.predict(X_val_fold)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_pred_fold))\n","\n","    fold += 1\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","\n","# Train final model\n","model_final = create_random_forest_model()\n","model_final.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = model_final.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(model_final, 'random_forest_model.pkl')\n","print(\"Model saved to random_forest_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MP5RyWOh6Cjv","executionInfo":{"status":"ok","timestamp":1731138887850,"user_tz":-180,"elapsed":154249,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"67ae9c44-7c88-4c9b-927d-b456095b388f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.98      0.96       771\n","         1.0       0.79      0.77      0.78       771\n","         2.0       0.82      0.80      0.81       771\n","         3.0       0.98      1.00      0.99       771\n","\n","    accuracy                           0.89      3084\n","   macro avg       0.89      0.89      0.89      3084\n","weighted avg       0.89      0.89      0.89      3084\n","\n","[[755  14   1   1]\n"," [ 45 591 131   4]\n"," [  4 140 620   7]\n"," [  1   0   0 770]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.98      0.96       771\n","         1.0       0.78      0.76      0.77       771\n","         2.0       0.80      0.79      0.80       771\n","         3.0       0.99      0.99      0.99       771\n","\n","    accuracy                           0.88      3084\n","   macro avg       0.88      0.88      0.88      3084\n","weighted avg       0.88      0.88      0.88      3084\n","\n","[[754  16   0   1]\n"," [ 40 584 144   3]\n"," [  5 150 609   7]\n"," [  0   0   4 767]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.97      0.96       771\n","         1.0       0.79      0.74      0.76       771\n","         2.0       0.80      0.82      0.81       771\n","         3.0       0.99      1.00      0.99       771\n","\n","    accuracy                           0.88      3084\n","   macro avg       0.88      0.88      0.88      3084\n","weighted avg       0.88      0.88      0.88      3084\n","\n","[[751  19   1   0]\n"," [ 44 567 159   1]\n"," [  2 130 633   6]\n"," [  0   0   2 769]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.36      0.46      0.41        28\n","         1.0       0.73      0.69      0.71       407\n","         2.0       0.79      0.83      0.81       579\n","         3.0       0.18      0.09      0.12        23\n","\n","    accuracy                           0.75      1037\n","   macro avg       0.52      0.52      0.51      1037\n","weighted avg       0.74      0.75      0.74      1037\n","\n","[[ 13  12   3   0]\n"," [ 17 281 109   0]\n"," [  4  86 480   9]\n"," [  2   4  15   2]]\n","Model saved to random_forest_model.pkl\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/stratified_test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,  # Number of trees in the forest\n","        criterion='gini',  # Function to measure the quality of a split\n","        max_depth=None,  # Maximum depth of the tree\n","        random_state=42,\n","        n_jobs=-1  # Use all available cores\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_random_forest_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the validation set\n","    y_pred_fold = model.predict(X_val_fold)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_pred_fold))\n","\n","    fold += 1\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","\n","# Train final model\n","model_final = create_random_forest_model()\n","model_final.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = model_final.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(model_final, 'random_forest_model.pkl')\n","print(\"Model saved to random_forest_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpY8Roi87wFM","executionInfo":{"status":"ok","timestamp":1731139385313,"user_tz":-180,"elapsed":235253,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"04ae80a1-3894-4a00-bec6-2cf5f98bbffa"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.98      0.95       232\n","         1.0       0.82      0.76      0.79       231\n","         2.0       0.84      0.84      0.84       231\n","         3.0       0.99      1.00      0.99       232\n","\n","    accuracy                           0.90       926\n","   macro avg       0.89      0.90      0.89       926\n","weighted avg       0.89      0.90      0.89       926\n","\n","[[228   3   0   1]\n"," [ 19 175  36   1]\n"," [  0  35 195   1]\n"," [  0   0   0 232]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.99      0.96       232\n","         1.0       0.81      0.75      0.78       231\n","         2.0       0.82      0.82      0.82       231\n","         3.0       0.99      1.00      0.99       232\n","\n","    accuracy                           0.89       926\n","   macro avg       0.89      0.89      0.89       926\n","weighted avg       0.89      0.89      0.89       926\n","\n","[[230   2   0   0]\n"," [ 14 174  42   1]\n"," [  2  38 189   2]\n"," [  0   0   0 232]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.97      0.98      0.98       232\n","         1.0       0.82      0.81      0.81       231\n","         2.0       0.83      0.83      0.83       231\n","         3.0       1.00      1.00      1.00       231\n","\n","    accuracy                           0.90       925\n","   macro avg       0.90      0.90      0.90       925\n","weighted avg       0.90      0.90      0.90       925\n","\n","[[228   4   0   0]\n"," [  6 186  39   0]\n"," [  0  38 192   1]\n"," [  1   0   0 230]]\n","Training fold 4...\n","Classification Report for fold 4:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.96       231\n","         1.0       0.80      0.75      0.78       232\n","         2.0       0.81      0.81      0.81       231\n","         3.0       0.98      1.00      0.99       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.88      0.89      0.88       925\n","weighted avg       0.88      0.89      0.88       925\n","\n","[[228   2   1   0]\n"," [ 15 174  42   1]\n"," [  0  41 187   3]\n"," [  0   0   0 231]]\n","Training fold 5...\n","Classification Report for fold 5:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.98      0.97       231\n","         1.0       0.81      0.78      0.79       232\n","         2.0       0.82      0.81      0.82       231\n","         3.0       0.99      1.00      0.99       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[227   4   0   0]\n"," [ 10 181  41   0]\n"," [  2  39 188   2]\n"," [  0   0   1 230]]\n","Training fold 6...\n","Classification Report for fold 6:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.96      0.98      0.97       231\n","         1.0       0.80      0.76      0.78       232\n","         2.0       0.80      0.81      0.80       231\n","         3.0       0.98      1.00      0.99       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[226   4   1   0]\n"," [  7 177  46   2]\n"," [  2  40 187   2]\n"," [  0   1   0 230]]\n","Training fold 7...\n","Classification Report for fold 7:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.99      0.95       231\n","         1.0       0.78      0.74      0.76       231\n","         2.0       0.83      0.80      0.81       232\n","         3.0       1.00      1.00      1.00       231\n","\n","    accuracy                           0.88       925\n","   macro avg       0.88      0.88      0.88       925\n","weighted avg       0.88      0.88      0.88       925\n","\n","[[228   3   0   0]\n"," [ 22 170  38   1]\n"," [  1  45 186   0]\n"," [  0   0   1 230]]\n","Training fold 8...\n","Classification Report for fold 8:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.98      0.96       231\n","         1.0       0.82      0.77      0.79       231\n","         2.0       0.82      0.84      0.83       232\n","         3.0       1.00      0.99      1.00       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[226   4   1   0]\n"," [ 14 177  40   0]\n"," [  1  36 195   0]\n"," [  0   0   2 229]]\n","Training fold 9...\n","Classification Report for fold 9:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.98      0.97       231\n","         1.0       0.79      0.79      0.79       231\n","         2.0       0.83      0.81      0.82       232\n","         3.0       0.99      1.00      1.00       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[226   5   0   0]\n"," [ 10 182  39   0]\n"," [  1  42 187   2]\n"," [  0   0   0 231]]\n","Training fold 10...\n","Classification Report for fold 10:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.96       231\n","         1.0       0.81      0.70      0.75       231\n","         2.0       0.77      0.83      0.80       231\n","         3.0       0.99      1.00      0.99       232\n","\n","    accuracy                           0.88       925\n","   macro avg       0.88      0.88      0.88       925\n","weighted avg       0.88      0.88      0.88       925\n","\n","[[229   2   0   0]\n"," [ 14 161  55   1]\n"," [  1  36 192   2]\n"," [  0   0   1 231]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.36      0.46      0.41        28\n","         1.0       0.73      0.69      0.71       407\n","         2.0       0.79      0.83      0.81       579\n","         3.0       0.18      0.09      0.12        23\n","\n","    accuracy                           0.75      1037\n","   macro avg       0.52      0.52      0.51      1037\n","weighted avg       0.74      0.75      0.74      1037\n","\n","[[ 13  12   3   0]\n"," [ 17 281 109   0]\n"," [  4  86 480   9]\n"," [  2   4  15   2]]\n","Model saved to random_forest_model.pkl\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","test_data = pd.read_csv('/content/stratified_test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,  # Number of trees in the forest\n","        criterion='gini',  # Function to measure the quality of a split\n","        max_depth=None,  # Maximum depth of the tree\n","        random_state=42,\n","        n_jobs=-1  # Use all available cores\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","val_accuracies = []\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_random_forest_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the training set\n","    y_train_pred_fold = model.predict(X_train_fold)\n","    train_accuracy = accuracy_score(y_train_fold, y_train_pred_fold)\n","    train_accuracies.append(train_accuracy)\n","\n","    # Evaluate the model on the validation set\n","    y_val_pred_fold = model.predict(X_val_fold)\n","    val_accuracy = accuracy_score(y_val_fold, y_val_pred_fold)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_val_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_val_pred_fold))\n","\n","    fold += 1\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","\n","# Train final model\n","model_final = create_random_forest_model()\n","model_final.fit(X_train_tfidf, y_train)\n","\n","# Get predictions on the test set\n","y_pred_test = model_final.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_pred_test))\n","\n","# Calculate and print average training and validation accuracies\n","average_train_accuracy = np.mean(train_accuracies)\n","average_val_accuracy = np.mean(val_accuracies)\n","print(f\"Average training accuracy: {average_train_accuracy}\")\n","print(f\"Average validation accuracy: {average_val_accuracy}\")\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(model_final, 'random_forest_model.pkl')\n","print(\"Model saved to random_forest_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUicqLBI-Qat","executionInfo":{"status":"ok","timestamp":1731140041426,"user_tz":-180,"elapsed":237920,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"4830d100-96ca-477e-fd64-59627047b43b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 2313 samples\n","Class 1: 2313 samples\n","Class 2: 2313 samples\n","Class 3: 2313 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.98      0.95       232\n","         1.0       0.82      0.76      0.79       231\n","         2.0       0.84      0.84      0.84       231\n","         3.0       0.99      1.00      0.99       232\n","\n","    accuracy                           0.90       926\n","   macro avg       0.89      0.90      0.89       926\n","weighted avg       0.89      0.90      0.89       926\n","\n","[[228   3   0   1]\n"," [ 19 175  36   1]\n"," [  0  35 195   1]\n"," [  0   0   0 232]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.99      0.96       232\n","         1.0       0.81      0.75      0.78       231\n","         2.0       0.82      0.82      0.82       231\n","         3.0       0.99      1.00      0.99       232\n","\n","    accuracy                           0.89       926\n","   macro avg       0.89      0.89      0.89       926\n","weighted avg       0.89      0.89      0.89       926\n","\n","[[230   2   0   0]\n"," [ 14 174  42   1]\n"," [  2  38 189   2]\n"," [  0   0   0 232]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.97      0.98      0.98       232\n","         1.0       0.82      0.81      0.81       231\n","         2.0       0.83      0.83      0.83       231\n","         3.0       1.00      1.00      1.00       231\n","\n","    accuracy                           0.90       925\n","   macro avg       0.90      0.90      0.90       925\n","weighted avg       0.90      0.90      0.90       925\n","\n","[[228   4   0   0]\n"," [  6 186  39   0]\n"," [  0  38 192   1]\n"," [  1   0   0 230]]\n","Training fold 4...\n","Classification Report for fold 4:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.96       231\n","         1.0       0.80      0.75      0.78       232\n","         2.0       0.81      0.81      0.81       231\n","         3.0       0.98      1.00      0.99       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.88      0.89      0.88       925\n","weighted avg       0.88      0.89      0.88       925\n","\n","[[228   2   1   0]\n"," [ 15 174  42   1]\n"," [  0  41 187   3]\n"," [  0   0   0 231]]\n","Training fold 5...\n","Classification Report for fold 5:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.98      0.97       231\n","         1.0       0.81      0.78      0.79       232\n","         2.0       0.82      0.81      0.82       231\n","         3.0       0.99      1.00      0.99       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[227   4   0   0]\n"," [ 10 181  41   0]\n"," [  2  39 188   2]\n"," [  0   0   1 230]]\n","Training fold 6...\n","Classification Report for fold 6:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.96      0.98      0.97       231\n","         1.0       0.80      0.76      0.78       232\n","         2.0       0.80      0.81      0.80       231\n","         3.0       0.98      1.00      0.99       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[226   4   1   0]\n"," [  7 177  46   2]\n"," [  2  40 187   2]\n"," [  0   1   0 230]]\n","Training fold 7...\n","Classification Report for fold 7:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.99      0.95       231\n","         1.0       0.78      0.74      0.76       231\n","         2.0       0.83      0.80      0.81       232\n","         3.0       1.00      1.00      1.00       231\n","\n","    accuracy                           0.88       925\n","   macro avg       0.88      0.88      0.88       925\n","weighted avg       0.88      0.88      0.88       925\n","\n","[[228   3   0   0]\n"," [ 22 170  38   1]\n"," [  1  45 186   0]\n"," [  0   0   1 230]]\n","Training fold 8...\n","Classification Report for fold 8:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.98      0.96       231\n","         1.0       0.82      0.77      0.79       231\n","         2.0       0.82      0.84      0.83       232\n","         3.0       1.00      0.99      1.00       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[226   4   1   0]\n"," [ 14 177  40   0]\n"," [  1  36 195   0]\n"," [  0   0   2 229]]\n","Training fold 9...\n","Classification Report for fold 9:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.98      0.97       231\n","         1.0       0.79      0.79      0.79       231\n","         2.0       0.83      0.81      0.82       232\n","         3.0       0.99      1.00      1.00       231\n","\n","    accuracy                           0.89       925\n","   macro avg       0.89      0.89      0.89       925\n","weighted avg       0.89      0.89      0.89       925\n","\n","[[226   5   0   0]\n"," [ 10 182  39   0]\n"," [  1  42 187   2]\n"," [  0   0   0 231]]\n","Training fold 10...\n","Classification Report for fold 10:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.96       231\n","         1.0       0.81      0.70      0.75       231\n","         2.0       0.77      0.83      0.80       231\n","         3.0       0.99      1.00      0.99       232\n","\n","    accuracy                           0.88       925\n","   macro avg       0.88      0.88      0.88       925\n","weighted avg       0.88      0.88      0.88       925\n","\n","[[229   2   0   0]\n"," [ 14 161  55   1]\n"," [  1  36 192   2]\n"," [  0   0   1 231]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.36      0.46      0.41        28\n","         1.0       0.73      0.69      0.71       407\n","         2.0       0.79      0.83      0.81       579\n","         3.0       0.18      0.09      0.12        23\n","\n","    accuracy                           0.75      1037\n","   macro avg       0.52      0.52      0.51      1037\n","weighted avg       0.74      0.75      0.74      1037\n","\n","[[ 13  12   3   0]\n"," [ 17 281 109   0]\n"," [  4  86 480   9]\n"," [  2   4  15   2]]\n","Average training accuracy: 0.9880266076443414\n","Average validation accuracy: 0.8902932695114121\n","Model saved to random_forest_model.pkl\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Load the dataset\n","data = pd.read_csv('/content/no tok +pre.csv')\n","\n","# Drop rows where 'description' or 'severity' is NaN\n","data = data.dropna(subset=['description', 'severity'])\n","\n","# Define the input and target attributes\n","X = data['description']\n","y = data['severity']\n","\n","# Perform the first stratified sampling to split the data into training and test sets\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n","for train_index, test_index in split.split(X, y):\n","    strat_train_set = data.iloc[train_index].copy()\n","    strat_test_set = data.iloc[test_index].copy()\n","\n","# Define input and target attributes for the training set\n","X_train = strat_train_set['description']\n","y_train = strat_train_set['severity']\n","\n","# Perform the second stratified sampling on the training set to create validation and training sets\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n","for train_index, val_index in split.split(X_train, y_train):\n","    final_train_set = strat_train_set.iloc[train_index].copy()\n","    strat_val_set = strat_train_set.iloc[val_index].copy()\n","\n","# Save the splits to separate CSV files\n","final_train_set.to_csv('/content/augmented_balanced_train_data.csv', index=False)\n","strat_val_set.to_csv('/content/stratified_val_data.csv', index=False)\n","strat_test_set.to_csv('/content/stratified_test_data.csv', index=False)\n","\n","print(\"Stratified sampling complete. Training data saved to /content/stratified_train_data.csv\")\n","print(\"Validation data saved to /content/stratified_val_data.csv\")\n","print(\"Test data saved to /content/stratified_test_data.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUBLqtOzB8XR","executionInfo":{"status":"ok","timestamp":1731141693185,"user_tz":-180,"elapsed":386,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"d44480a0-72c1-4df0-9a09-05050a0ad2ce"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Stratified sampling complete. Training data saved to /content/stratified_train_data.csv\n","Validation data saved to /content/stratified_val_data.csv\n","Test data saved to /content/stratified_test_data.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","val_data = pd.read_csv('/content/stratified_val_data.csv')\n","test_data = pd.read_csv('/content/stratified_test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","val_data['description'] = val_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_val_tfidf = tfidf_vectorizer.transform(val_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_val = label_encoder.transform(val_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,  # Number of trees in the forest\n","        criterion='gini',  # Function to measure the quality of a split\n","        max_depth=None,  # Maximum depth of the tree\n","        random_state=42,\n","        n_jobs=-1  # Use all available cores\n","    )\n","    return model\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","val_accuracies = []\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Create the model\n","    model = create_random_forest_model()\n","\n","    # Train the model\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the training set\n","    y_train_pred_fold = model.predict(X_train_fold)\n","    train_accuracy = accuracy_score(y_train_fold, y_train_pred_fold)\n","    train_accuracies.append(train_accuracy)\n","\n","    # Evaluate the model on the validation set\n","    y_val_pred_fold = model.predict(X_val_fold)\n","    val_accuracy = accuracy_score(y_val_fold, y_val_pred_fold)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_val_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_val_pred_fold))\n","\n","    fold += 1\n","\n","# Train final model on full training data\n","final_model = create_random_forest_model()\n","final_model.fit(X_train_tfidf, y_train)\n","\n","# Evaluate the model on the validation set\n","y_val_pred = final_model.predict(X_val_tfidf)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation accuracy: {val_accuracy}\")\n","\n","# Print classification report and confusion matrix for the validation set\n","print(f\"Classification Report for validation set:\")\n","print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_val, y_val_pred))\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","y_test_pred = final_model.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_test_pred))\n","\n","# Calculate and print average training and validation accuracies\n","average_train_accuracy = np.mean(train_accuracies)\n","average_val_accuracy = np.mean(val_accuracies)\n","print(f\"Average training accuracy: {average_train_accuracy}\")\n","print(f\"Average validation accuracy: {average_val_accuracy}\")\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(final_model, 'random_forest_model.pkl')\n","print(\"Model saved to random_forest_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxD9_c4dG92o","executionInfo":{"status":"ok","timestamp":1731142692908,"user_tz":-180,"elapsed":196383,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"3052568d-51c4-4dd8-8f2f-eda5a61e2556"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 1850 samples\n","Class 1: 1850 samples\n","Class 2: 1850 samples\n","Class 3: 1850 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.98      0.95       185\n","         1.0       0.80      0.71      0.76       185\n","         2.0       0.82      0.84      0.83       185\n","         3.0       0.99      1.00      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[181   4   0   0]\n"," [ 17 132  35   1]\n"," [  0  28 156   1]\n"," [  0   0   0 185]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.98      0.97       185\n","         1.0       0.81      0.74      0.77       185\n","         2.0       0.79      0.83      0.81       185\n","         3.0       1.00      1.00      1.00       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[182   2   1   0]\n"," [  9 137  39   0]\n"," [  1  31 153   0]\n"," [  0   0   0 185]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.98      0.96       185\n","         1.0       0.75      0.69      0.72       185\n","         2.0       0.75      0.77      0.76       185\n","         3.0       0.98      0.99      0.99       185\n","\n","    accuracy                           0.86       740\n","   macro avg       0.86      0.86      0.86       740\n","weighted avg       0.86      0.86      0.86       740\n","\n","[[182   2   1   0]\n"," [ 11 127  46   1]\n"," [  1  40 142   2]\n"," [  0   0   1 184]]\n","Training fold 4...\n","Classification Report for fold 4:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.99      0.95       185\n","         1.0       0.78      0.74      0.76       185\n","         2.0       0.81      0.78      0.80       185\n","         3.0       0.99      0.99      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.87      0.88      0.87       740\n","weighted avg       0.87      0.88      0.87       740\n","\n","[[184   1   0   0]\n"," [ 16 136  32   1]\n"," [  2  38 144   1]\n"," [  0   0   1 184]]\n","Training fold 5...\n","Classification Report for fold 5:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.96       185\n","         1.0       0.84      0.74      0.78       185\n","         2.0       0.80      0.84      0.82       185\n","         3.0       0.97      1.00      0.99       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[183   2   0   0]\n"," [ 11 136  38   0]\n"," [  1  24 155   5]\n"," [  0   0   0 185]]\n","Training fold 6...\n","Classification Report for fold 6:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.99      0.96       185\n","         1.0       0.78      0.75      0.76       185\n","         2.0       0.81      0.78      0.79       185\n","         3.0       0.99      0.99      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[184   1   0   0]\n"," [ 13 139  33   0]\n"," [  1  39 144   1]\n"," [  0   0   1 184]]\n","Training fold 7...\n","Classification Report for fold 7:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.98      0.96       185\n","         1.0       0.79      0.79      0.79       185\n","         2.0       0.84      0.79      0.82       185\n","         3.0       0.99      0.99      0.99       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[181   4   0   0]\n"," [ 11 147  27   0]\n"," [  2  35 147   1]\n"," [  0   0   1 184]]\n","Training fold 8...\n","Classification Report for fold 8:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.97      0.95       185\n","         1.0       0.80      0.77      0.78       185\n","         2.0       0.83      0.83      0.83       185\n","         3.0       0.99      1.00      1.00       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[179   6   0   0]\n"," [ 12 142  31   0]\n"," [  1  29 154   1]\n"," [  0   0   0 185]]\n","Training fold 9...\n","Classification Report for fold 9:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.99      0.96       185\n","         1.0       0.79      0.74      0.76       185\n","         2.0       0.82      0.81      0.82       185\n","         3.0       0.99      1.00      1.00       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.88      0.89      0.88       740\n","weighted avg       0.88      0.89      0.88       740\n","\n","[[183   2   0   0]\n"," [ 15 137  32   1]\n"," [  0  35 150   0]\n"," [  0   0   0 185]]\n","Training fold 10...\n","Classification Report for fold 10:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.98      0.96       185\n","         1.0       0.82      0.75      0.78       185\n","         2.0       0.83      0.84      0.84       185\n","         3.0       0.99      1.00      1.00       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[182   3   0   0]\n"," [ 13 139  32   1]\n"," [  1  28 156   0]\n"," [  0   0   0 185]]\n","Validation accuracy: 0.7433734939759036\n","Classification Report for validation set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.31      0.48      0.38        23\n","         1.0       0.70      0.70      0.70       326\n","         2.0       0.80      0.81      0.81       463\n","         3.0       0.67      0.22      0.33        18\n","\n","    accuracy                           0.74       830\n","   macro avg       0.62      0.55      0.56       830\n","weighted avg       0.75      0.74      0.74       830\n","\n","[[ 11  10   2   0]\n"," [ 19 228  79   0]\n"," [  4  83 374   2]\n"," [  1   3  10   4]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.37      0.50      0.42        28\n","         1.0       0.73      0.69      0.71       407\n","         2.0       0.79      0.82      0.81       579\n","         3.0       0.20      0.09      0.12        23\n","\n","    accuracy                           0.75      1037\n","   macro avg       0.52      0.52      0.51      1037\n","weighted avg       0.74      0.75      0.74      1037\n","\n","[[ 14  11   3   0]\n"," [ 17 280 110   0]\n"," [  5  89 477   8]\n"," [  2   6  13   2]]\n","Average training accuracy: 0.988963963963964\n","Average validation accuracy: 0.8837837837837839\n","Model saved to random_forest_model.pkl\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tfeyEJW5QCYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import xgboost as xgb\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/augmented_balanced_train_data.csv')\n","val_data = pd.read_csv('/content/stratified_val_data.csv')\n","test_data = pd.read_csv('/content/stratified_test_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","val_data['description'] = val_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_val_tfidf = tfidf_vectorizer.transform(val_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_val = label_encoder.transform(val_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,\n","        criterion='gini',\n","        max_depth=None,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","    return model\n","\n","# Define the XGBoost model\n","def create_xgboost_model():\n","    model = xgb.XGBClassifier(\n","        n_estimators=100,\n","        max_depth=6,\n","        learning_rate=0.1,\n","        objective='multi:softmax',\n","        num_class=len(np.unique(y_train)),\n","        random_state=42\n","    )\n","    return model\n","\n","# Create the ensemble model\n","ensemble_model = VotingClassifier(\n","    estimators=[\n","        ('rf', create_random_forest_model()),\n","        ('xgb', create_xgboost_model())\n","    ],\n","    voting='soft'  # Use soft voting for probability-based voting\n",")\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","val_accuracies = []\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Train the ensemble model\n","    ensemble_model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the training set\n","    y_train_pred_fold = ensemble_model.predict(X_train_fold)\n","    train_accuracy = accuracy_score(y_train_fold, y_train_pred_fold)\n","    train_accuracies.append(train_accuracy)\n","\n","    # Evaluate the model on the validation set\n","    y_val_pred_fold = ensemble_model.predict(X_val_fold)\n","    val_accuracy = accuracy_score(y_val_fold, y_val_pred_fold)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_val_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_val_pred_fold))\n","\n","    fold += 1\n","\n","# Train final model on full training data\n","final_model = VotingClassifier(\n","    estimators=[\n","        ('rf', create_random_forest_model()),\n","        ('xgb', create_xgboost_model())\n","    ],\n","    voting='soft'\n",")\n","final_model.fit(X_train_tfidf, y_train)\n","\n","# Evaluate the model on the validation set\n","y_val_pred = final_model.predict(X_val_tfidf)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation accuracy: {val_accuracy}\")\n","\n","# Print classification report and confusion matrix for the validation set\n","print(f\"Classification Report for validation set:\")\n","print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_val, y_val_pred))\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","y_test_pred = final_model.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_test_pred))\n","\n","# Calculate and print average training and validation accuracies\n","average_train_accuracy = np.mean(train_accuracies)\n","average_val_accuracy = np.mean(val_accuracies)\n","print(f\"Average training accuracy: {average_train_accuracy}\")\n","print(f\"Average validation accuracy: {average_val_accuracy}\")\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(final_model, 'ensemble_model.pkl')\n","print(\"Model saved to ensemble_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_gXUnXWKuPV","executionInfo":{"status":"ok","timestamp":1731143644008,"user_tz":-180,"elapsed":562757,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"3e0a3f4d-8a72-46c6-dc02-7908be36ce85"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 1850 samples\n","Class 1: 1850 samples\n","Class 2: 1850 samples\n","Class 3: 1850 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.98      0.95       185\n","         1.0       0.80      0.69      0.74       185\n","         2.0       0.80      0.82      0.81       185\n","         3.0       0.97      1.00      0.98       185\n","\n","    accuracy                           0.87       740\n","   macro avg       0.87      0.87      0.87       740\n","weighted avg       0.87      0.87      0.87       740\n","\n","[[182   3   0   0]\n"," [ 18 128  37   2]\n"," [  0  29 152   4]\n"," [  0   0   0 185]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.99      0.95       185\n","         1.0       0.83      0.74      0.78       185\n","         2.0       0.80      0.80      0.80       185\n","         3.0       0.97      1.00      0.98       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[184   0   1   0]\n"," [ 12 136  36   1]\n"," [  5  27 148   5]\n"," [  0   0   0 185]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.98      0.96       185\n","         1.0       0.74      0.69      0.71       185\n","         2.0       0.75      0.75      0.75       185\n","         3.0       0.96      1.00      0.98       185\n","\n","    accuracy                           0.85       740\n","   macro avg       0.85      0.85      0.85       740\n","weighted avg       0.85      0.85      0.85       740\n","\n","[[182   0   3   0]\n"," [  9 127  43   6]\n"," [  2  44 138   1]\n"," [  0   0   0 185]]\n","Training fold 4...\n","Classification Report for fold 4:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.90      0.99      0.94       185\n","         1.0       0.78      0.75      0.76       185\n","         2.0       0.83      0.75      0.79       185\n","         3.0       0.96      0.99      0.98       185\n","\n","    accuracy                           0.87       740\n","   macro avg       0.87      0.87      0.87       740\n","weighted avg       0.87      0.87      0.87       740\n","\n","[[184   1   0   0]\n"," [ 15 138  28   4]\n"," [  6  37 139   3]\n"," [  0   0   1 184]]\n","Training fold 5...\n","Classification Report for fold 5:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.99      0.95       185\n","         1.0       0.85      0.74      0.79       185\n","         2.0       0.82      0.82      0.82       185\n","         3.0       0.96      1.00      0.98       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.88      0.89      0.88       740\n","weighted avg       0.88      0.89      0.88       740\n","\n","[[183   2   0   0]\n"," [ 15 137  33   0]\n"," [  3  23 151   8]\n"," [  0   0   0 185]]\n","Training fold 6...\n","Classification Report for fold 6:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.99      0.95       185\n","         1.0       0.80      0.74      0.77       185\n","         2.0       0.82      0.81      0.81       185\n","         3.0       0.98      0.99      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[183   1   1   0]\n"," [ 14 136  31   4]\n"," [  3  33 149   0]\n"," [  0   0   1 184]]\n","Training fold 7...\n","Classification Report for fold 7:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.99      0.96       185\n","         1.0       0.81      0.79      0.80       185\n","         2.0       0.86      0.80      0.83       185\n","         3.0       0.97      0.99      0.98       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[183   2   0   0]\n"," [ 12 147  24   2]\n"," [  1  33 148   3]\n"," [  0   0   1 184]]\n","Training fold 8...\n","Classification Report for fold 8:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.99      0.96       185\n","         1.0       0.82      0.76      0.79       185\n","         2.0       0.85      0.81      0.83       185\n","         3.0       0.95      1.00      0.98       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[183   2   0   0]\n"," [ 14 141  27   3]\n"," [  1  28 150   6]\n"," [  0   0   0 185]]\n","Training fold 9...\n","Classification Report for fold 9:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.98      0.95       185\n","         1.0       0.77      0.71      0.74       185\n","         2.0       0.80      0.79      0.80       185\n","         3.0       0.98      1.00      0.99       185\n","\n","    accuracy                           0.87       740\n","   macro avg       0.87      0.87      0.87       740\n","weighted avg       0.87      0.87      0.87       740\n","\n","[[182   3   0   0]\n"," [ 16 132  36   1]\n"," [  1  36 146   2]\n"," [  0   0   0 185]]\n","Training fold 10...\n","Classification Report for fold 10:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.98      0.95       185\n","         1.0       0.83      0.76      0.79       185\n","         2.0       0.83      0.83      0.83       185\n","         3.0       0.99      0.99      0.99       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[182   2   1   0]\n"," [ 15 141  29   0]\n"," [  3  27 153   2]\n"," [  0   0   1 184]]\n","Validation accuracy: 0.736144578313253\n","Classification Report for validation set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.27      0.48      0.34        23\n","         1.0       0.71      0.72      0.72       326\n","         2.0       0.81      0.77      0.79       463\n","         3.0       0.38      0.33      0.35        18\n","\n","    accuracy                           0.74       830\n","   macro avg       0.54      0.58      0.55       830\n","weighted avg       0.75      0.74      0.74       830\n","\n","[[ 11  10   2   0]\n"," [ 19 236  71   0]\n"," [ 10  85 358  10]\n"," [  1   2   9   6]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.36      0.57      0.44        28\n","         1.0       0.71      0.67      0.69       407\n","         2.0       0.80      0.80      0.80       579\n","         3.0       0.17      0.17      0.17        23\n","\n","    accuracy                           0.73      1037\n","   macro avg       0.51      0.55      0.53      1037\n","weighted avg       0.74      0.73      0.73      1037\n","\n","[[ 16  10   2   0]\n"," [ 23 274 105   5]\n"," [  3  98 464  14]\n"," [  2   5  12   4]]\n","Average training accuracy: 0.9774324324324326\n","Average validation accuracy: 0.8798648648648649\n","Model saved to ensemble_model.pkl\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n","from collections import Counter\n","\n","# Load the datasets\n","train_data = pd.read_csv('/content/strat augmented_balanced_train_data.csv')\n","val_data = pd.read_csv('/content/stratified_val_data.csv')\n","test_data = pd.read_csv('/content/stratified_val_data.csv')\n","\n","# Initialize spaCy and tokenize the text\n","nlp = spacy.load('en_core_web_sm')\n","\n","def tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n","\n","# Tokenize the descriptions\n","train_data['description'] = train_data['description'].apply(tokenize_text)\n","val_data['description'] = val_data['description'].apply(tokenize_text)\n","test_data['description'] = test_data['description'].apply(tokenize_text)\n","\n","# Encode the descriptions using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['description']).toarray()\n","X_val_tfidf = tfidf_vectorizer.transform(val_data['description']).toarray()\n","X_test_tfidf = tfidf_vectorizer.transform(test_data['description']).toarray()\n","\n","# Encode the target variable (severity)\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['severity'])\n","y_val = label_encoder.transform(val_data['severity'])\n","y_test = label_encoder.transform(test_data['severity'])\n","\n","# Check class distribution\n","class_distribution = Counter(y_train)\n","print(\"Class distribution in the training data:\")\n","for class_label, count in class_distribution.items():\n","    print(f\"Class {class_label}: {count} samples\")\n","\n","# Define the Random Forest model\n","def create_random_forest_model():\n","    model = RandomForestClassifier(\n","        n_estimators=100,  # Number of trees in the forest\n","        criterion='gini',  # Function to measure the quality of a split\n","        max_depth=None,  # Maximum depth of the tree\n","        random_state=42,\n","        n_jobs=-1  # Use all available cores\n","    )\n","    return model\n","\n","# Create the Bagging model with Random Forest\n","bagging_model = BaggingClassifier(\n","    estimator=create_random_forest_model(),\n","    n_estimators=10,  # Number of base estimators in the ensemble\n","    random_state=42\n",")\n","\n","# Perform cross-validation using StratifiedKFold\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","val_accuracies = []\n","\n","fold = 1\n","for train_index, val_index in kf.split(X_train_tfidf, y_train):\n","    print(f\"Training fold {fold}...\")\n","\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","    # Train the Bagging model\n","    bagging_model.fit(X_train_fold, y_train_fold)\n","\n","    # Evaluate the model on the training set\n","    y_train_pred_fold = bagging_model.predict(X_train_fold)\n","    train_accuracy = accuracy_score(y_train_fold, y_train_pred_fold)\n","    train_accuracies.append(train_accuracy)\n","\n","    # Evaluate the model on the validation set\n","    y_val_pred_fold = bagging_model.predict(X_val_fold)\n","    val_accuracy = accuracy_score(y_val_fold, y_val_pred_fold)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Print classification report and confusion matrix for this fold\n","    print(f\"Classification Report for fold {fold}:\")\n","    print(classification_report(y_val_fold, y_val_pred_fold, target_names=label_encoder.classes_.astype(str)))\n","    print(confusion_matrix(y_val_fold, y_val_pred_fold))\n","\n","    fold += 1\n","\n","# Train final model on full training data\n","bagging_model.fit(X_train_tfidf, y_train)\n","\n","# Evaluate the model on the validation set\n","y_val_pred = bagging_model.predict(X_val_tfidf)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation accuracy: {val_accuracy}\")\n","\n","# Print classification report and confusion matrix for the validation set\n","print(f\"Classification Report for validation set:\")\n","print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_val, y_val_pred))\n","\n","# Final evaluation on the test set\n","print(\"Final evaluation on the test set:\")\n","y_test_pred = bagging_model.predict(X_test_tfidf)\n","\n","# Print classification report and confusion matrix on the test set\n","print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n","print(confusion_matrix(y_test, y_test_pred))\n","\n","# Calculate and print average training and validation accuracies\n","average_train_accuracy = np.mean(train_accuracies)\n","average_val_accuracy = np.mean(val_accuracies)\n","print(f\"Average training accuracy: {average_train_accuracy}\")\n","print(f\"Average validation accuracy: {average_val_accuracy}\")\n","\n","# Optionally: Save the model for later use\n","import joblib\n","joblib.dump(bagging_model, 'random_forest_bagging_model.pkl')\n","print(\"Model saved to random_forest_bagging_model.pkl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6VPsN0XQDQU","executionInfo":{"status":"ok","timestamp":1731154006112,"user_tz":-180,"elapsed":610924,"user":{"displayName":"Shambel Gashaw","userId":"10044554980528052762"}},"outputId":"272ef6b1-b257-4e6b-bb06-1ca1ffed5356"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the training data:\n","Class 0: 1850 samples\n","Class 1: 1850 samples\n","Class 2: 1850 samples\n","Class 3: 1850 samples\n","Training fold 1...\n","Classification Report for fold 1:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      1.00      0.95       185\n","         1.0       0.81      0.70      0.75       185\n","         2.0       0.81      0.83      0.82       185\n","         3.0       0.98      1.00      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[185   0   0   0]\n"," [ 18 130  36   1]\n"," [  0  30 153   2]\n"," [  0   0   0 185]]\n","Training fold 2...\n","Classification Report for fold 2:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.97       185\n","         1.0       0.81      0.72      0.76       185\n","         2.0       0.78      0.83      0.80       185\n","         3.0       1.00      1.00      1.00       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[183   1   1   0]\n"," [ 10 133  42   0]\n"," [  1  31 153   0]\n"," [  0   0   0 185]]\n","Training fold 3...\n","Classification Report for fold 3:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.97       185\n","         1.0       0.72      0.70      0.71       185\n","         2.0       0.74      0.71      0.72       185\n","         3.0       0.98      1.00      0.99       185\n","\n","    accuracy                           0.85       740\n","   macro avg       0.84      0.85      0.85       740\n","weighted avg       0.84      0.85      0.85       740\n","\n","[[183   1   1   0]\n"," [ 10 129  45   1]\n"," [  1  50 131   3]\n"," [  0   0   0 185]]\n","Training fold 4...\n","Classification Report for fold 4:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      1.00      0.96       185\n","         1.0       0.78      0.75      0.77       185\n","         2.0       0.83      0.78      0.80       185\n","         3.0       0.99      0.99      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[185   0   0   0]\n"," [ 16 139  29   1]\n"," [  1  39 144   1]\n"," [  0   0   1 184]]\n","Training fold 5...\n","Classification Report for fold 5:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.99      0.97       185\n","         1.0       0.86      0.74      0.79       185\n","         2.0       0.80      0.84      0.82       185\n","         3.0       0.97      1.00      0.98       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[184   0   1   0]\n"," [ 11 136  38   0]\n"," [  1  23 155   6]\n"," [  0   0   0 185]]\n","Training fold 6...\n","Classification Report for fold 6:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.99      0.96       185\n","         1.0       0.77      0.76      0.76       185\n","         2.0       0.80      0.77      0.78       185\n","         3.0       0.99      0.99      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[184   1   0   0]\n"," [ 12 140  33   0]\n"," [  1  41 142   1]\n"," [  0   0   2 183]]\n","Training fold 7...\n","Classification Report for fold 7:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.98      0.96       185\n","         1.0       0.79      0.80      0.79       185\n","         2.0       0.84      0.78      0.81       185\n","         3.0       0.99      1.00      0.99       185\n","\n","    accuracy                           0.89       740\n","   macro avg       0.89      0.89      0.89       740\n","weighted avg       0.89      0.89      0.89       740\n","\n","[[182   3   0   0]\n"," [ 10 148  27   0]\n"," [  2  37 144   2]\n"," [  0   0   0 185]]\n","Training fold 8...\n","Classification Report for fold 8:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.98      0.96       185\n","         1.0       0.80      0.72      0.76       185\n","         2.0       0.79      0.83      0.81       185\n","         3.0       0.99      0.99      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[182   3   0   0]\n"," [ 13 133  39   0]\n"," [  1  30 153   1]\n"," [  0   0   1 184]]\n","Training fold 9...\n","Classification Report for fold 9:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.99      0.96       185\n","         1.0       0.80      0.73      0.76       185\n","         2.0       0.81      0.81      0.81       185\n","         3.0       0.99      1.00      0.99       185\n","\n","    accuracy                           0.88       740\n","   macro avg       0.88      0.88      0.88       740\n","weighted avg       0.88      0.88      0.88       740\n","\n","[[184   0   1   0]\n"," [ 15 135  34   1]\n"," [  1  34 149   1]\n"," [  0   0   0 185]]\n","Training fold 10...\n","Classification Report for fold 10:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.99      0.96       185\n","         1.0       0.84      0.76      0.80       185\n","         2.0       0.84      0.85      0.85       185\n","         3.0       0.99      1.00      1.00       185\n","\n","    accuracy                           0.90       740\n","   macro avg       0.90      0.90      0.90       740\n","weighted avg       0.90      0.90      0.90       740\n","\n","[[184   1   0   0]\n"," [ 14 140  30   1]\n"," [  1  26 158   0]\n"," [  0   0   0 185]]\n","Validation accuracy: 0.7373493975903614\n","Classification Report for validation set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.30      0.48      0.37        23\n","         1.0       0.70      0.69      0.69       326\n","         2.0       0.80      0.80      0.80       463\n","         3.0       0.67      0.22      0.33        18\n","\n","    accuracy                           0.74       830\n","   macro avg       0.62      0.55      0.55       830\n","weighted avg       0.74      0.74      0.74       830\n","\n","[[ 11  10   2   0]\n"," [ 20 225  81   0]\n"," [  5  84 372   2]\n"," [  1   4   9   4]]\n","Final evaluation on the test set:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.30      0.48      0.37        23\n","         1.0       0.70      0.69      0.69       326\n","         2.0       0.80      0.80      0.80       463\n","         3.0       0.67      0.22      0.33        18\n","\n","    accuracy                           0.74       830\n","   macro avg       0.62      0.55      0.55       830\n","weighted avg       0.74      0.74      0.74       830\n","\n","[[ 11  10   2   0]\n"," [ 20 225  81   0]\n"," [  5  84 372   2]\n"," [  1   4   9   4]]\n","Average training accuracy: 0.9781981981981982\n","Average validation accuracy: 0.8820270270270271\n","Model saved to random_forest_bagging_model.pkl\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMD8PxAskJFyAg5/U9nYDkA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}